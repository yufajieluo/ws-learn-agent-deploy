import os
import streamlit as st
import tempfile
import uuid
import shutil
from dotenv import load_dotenv

# --- LangChain ç»„ä»¶ ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

# --- âš™ï¸ 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="Jarvis: Time Traveler", layout="wide", page_icon="ğŸ•°ï¸")
st.title("ğŸ•°ï¸ Jarvis: æ”¯æŒ'æ—¶é—´æ—…è¡Œ'çš„ RAG åŠ©æ‰‹")

# --- ğŸ” 2. Session ID ç®¡ç† (æ ¸å¿ƒä¿®æ”¹) ---

# åˆå§‹åŒ–ä¸€ä¸ªé»˜è®¤çš„éšæœº ID (ä»…åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ç”Ÿæˆ)
if "init_id" not in st.session_state:
    st.session_state.init_id = str(uuid.uuid4())[:8] # å–å‰8ä½æ–¹ä¾¿è¾“å…¥

# --- ğŸ¨ 3. ä¾§è¾¹æ æ§åˆ¶ä¸­å¿ƒ ---
with st.sidebar:
    st.header("ğŸ® æ§åˆ¶å°")
    
    # API Key
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        api_key = st.text_input("Google API Key", type="password")

    st.divider()

    # ğŸ”¥ A. æ‰‹åŠ¨ Session ID è¾“å…¥æ¡†
    st.subheader("ğŸ†” ä¼šè¯ç®¡ç† (Session)")
    
    # è¿™é‡Œæ˜¯å…³é”®ï¼šè¾“å…¥æ¡†çš„å€¼å†³å®šäº†å½“å‰çš„ session_id
    # é»˜è®¤å€¼æ˜¯é‚£ä¸ªéšæœºç”Ÿæˆçš„ï¼Œä½†ä½ å¯ä»¥æ‰‹åŠ¨æ”¹æˆ "test", "demo", "user1" ç­‰
    custom_session_id = st.text_input(
        "å½“å‰ Session ID", 
        value=st.session_state.init_id,
        help="ä¿®æ”¹æ­¤ ID å¯åˆ‡æ¢ä¸åŒçš„å¯¹è¯å†å²ã€‚è¾“å…¥æ—§çš„ ID å¯ä»¥æ‰¾å›è®°å¿†ã€‚"
    )

    # ç®€å•çš„çŠ¶æ€ç›‘æµ‹
    if "last_session_id" not in st.session_state:
        st.session_state.last_session_id = custom_session_id
    
    # ğŸ’¡ ç›‘æµ‹ ID å˜åŒ–ï¼šå¦‚æœç”¨æˆ·åˆ‡äº† IDï¼Œæˆ‘ä»¬è¦æ¸…ç† RAG ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢æ•°æ®ä¸²å‘³
    if custom_session_id != st.session_state.last_session_id:
        st.toast(f"ğŸ”„ åˆ‡æ¢ä¼šè¯: {st.session_state.last_session_id} -> {custom_session_id}")
        st.session_state.last_session_id = custom_session_id
        # æ¸…é™¤æ—§çš„ Retriever (RAG)
        if "retriever" in st.session_state:
            del st.session_state.retriever
        st.rerun()

    st.divider()

    # ğŸ”¥ B. è‡ªå®šä¹‰äººè®¾
    system_persona = st.text_area(
        "ğŸ­ ç³»ç»Ÿäººè®¾ (System Prompt)",
        value="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
        height=100
    )

    # ğŸ”¥ C. RAG æ–‡æ¡£
    st.subheader("ğŸ“š çŸ¥è¯†åº“ (RAG)")
    uploaded_file = st.file_uploader("ä¸Šä¼ å½“å‰ä¼šè¯çš„æ–‡æ¡£ (PDF/TXT)", type=["pdf", "txt"])
    
    # æ¸…ç©ºå†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰ ID çš„å†å²"):
        # è¿æ¥æ•°æ®åº“å¹¶æ¸…ç©ºæŒ‡å®š Session çš„è®°å½•
        history_db = SQLChatMessageHistory(
            session_id=custom_session_id,
            connection="sqlite:///chat_history.db"
        )
        history_db.clear()
        st.toast("å†å²è®°å½•å·²æŠ¹é™¤ï¼")
        st.rerun()

# --- ğŸ§  4. æ ¸å¿ƒé€»è¾‘ ---

if not api_key:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§é…ç½® API Key")
    st.stop()

# å®šä¹‰è·å–å†å²è®°å½•çš„å‡½æ•° (LangChain éœ€è¦è¿™ä¸ª)
def get_session_history(session_id):
    """æ ¹æ® session_id ä» SQLite è¯»å–å†å²"""
    return SQLChatMessageHistory(
        session_id=session_id,
        connection="sqlite:///chat_history.db"
    )

# å¤„ç†æ–‡ä»¶ä¸Šä¼ 
def process_file(uploaded_file, session_id):
    """å¤„ç†æ–‡ä»¶å¹¶å­˜å…¥éš”ç¦»çš„å‘é‡åº“"""
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    loader = PyPDFLoader(tmp_path) if uploaded_file.name.endswith('.pdf') else TextLoader(tmp_path)
    docs = loader.load()
    os.remove(tmp_path)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001", google_api_key=api_key)
    
    # ä½¿ç”¨ session_id ä½œä¸º collection nameï¼Œå®ç° RAG ç‰©ç†éš”ç¦»
    # æ³¨æ„ï¼šChroma collection name åªèƒ½åŒ…å«å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œä¸”ä¸èƒ½å¤ªé•¿
    safe_collection_name = f"rag_{session_id}".replace("-", "_")
    
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings,
        collection_name=safe_collection_name, 
        persist_directory="./chroma_db"
    )
    return vectorstore.as_retriever()

# --- ğŸ”— 5. æ„å»º Chain ---

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=api_key,
    temperature=0.7
)

# é€»è¾‘åˆ†æµï¼šæœ‰æ–‡ä»¶ vs æ— æ–‡ä»¶
if uploaded_file:
    # åªæœ‰å½“ retriever ä¸å­˜åœ¨æ—¶æ‰å¤„ç†ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–° Embedding
    if "retriever" not in st.session_state:
        with st.spinner("æ­£åœ¨å‘é‡åŒ–æ–‡æ¡£..."):
            st.session_state.retriever = process_file(uploaded_file, custom_session_id)
    
    retriever = st.session_state.retriever
    
    # RAG Prompt
    rag_system_prompt = (
        f"{system_persona}\n\n"
        "ã€æŒ‡ä»¤ã€‘ï¼šè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚\n"
        "ã€ä¸Šä¸‹æ–‡ã€‘:\n{context}"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", rag_system_prompt),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ])
    
    # è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–æ–‡æ¡£
    def format_docs(docs):
        return "\n\n".join(d.page_content for d in docs)

    # RAG Chain
    chain = (
        RunnablePassthrough.assign(
            context=lambda x: format_docs(retriever.invoke(x["question"]))
        )
        | prompt
        | llm
        | StrOutputParser()
    )
else:
    # æ™®é€šå¯¹è¯ Chain
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_persona),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ])
    chain = prompt | llm | StrOutputParser()

# æ³¨å…¥å†å²è®°å½•èƒ½åŠ›
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="history",
)

# --- ğŸ’¬ 6. èŠå¤©ç•Œé¢ ---

st.caption(f"å½“å‰æ­£åœ¨å¯¹è¯çš„ Session ID: **{custom_session_id}**")

# A. æ¸²æŸ“å†å²è®°å½• (ä» SQLite è¯»å–)
# æˆ‘ä»¬ç›´æ¥è°ƒç”¨ get_session_history æ¥è·å–å½“å‰ ID çš„å†å²
current_history = get_session_history(custom_session_id)
if not current_history.messages:
    st.info("ğŸ‘‹ è¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¼šè¯ï¼ˆæˆ–è€…å†å²è®°å½•ä¸ºç©ºï¼‰ã€‚")

for msg in current_history.messages:
    # ç®€å•çš„æ ·å¼æ˜ å°„
    role = "user" if msg.type == "human" else "assistant"
    with st.chat_message(role):
        st.markdown(msg.content)

# B. å¤„ç†è¾“å…¥
if user_input := st.chat_input("è¯´ç‚¹ä»€ä¹ˆ..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # 2. è°ƒç”¨ AI (æµå¼)
    with st.chat_message("assistant"):
        response_container = st.empty()
        full_response = ""
        
        # RunnableWithMessageHistory éœ€è¦ configurable å‚æ•°æ¥æŒ‡å®š session_id
        config = {"configurable": {"session_id": custom_session_id}}
        
        stream = chain_with_history.stream(
            {"question": user_input},
            config=config
        )
        
        for chunk in stream:
            full_response += chunk
            response_container.markdown(full_response + "â–Œ")
        
        response_container.markdown(full_response)